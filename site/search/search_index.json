{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"index.html","title":"Home","text":""},{"location":"index.html#welcome-to-the-sidra-bioinformatics-data-platform-documentation","title":"Welcome to the Sidra BioInformatics Data Platform Documentation","text":"<p>This platform is a custom solution developed to assist in tracking various genome samples. It meticulously records essential details, such as the date a sample was requested for sequencing, the stages of the entire pipeline until completion, and enables visualization of related metadata, including reference genomes and sample status.</p>"},{"location":"index.html#target-audience","title":"Target Audience","text":"<p>This documentation is intended for end-users of the platform, who may also be potential future developers.</p>"},{"location":"index.html#core-functionalities","title":"Core Functionalities","text":"<ul> <li>Data Extraction: The platform extracts data from specified files, directories, and filepaths, saving this information to a PostgreSQL database. This extraction process is read-only, ensuring the integrity of the original data sources.</li> <li>Web Interface: A user-friendly web interface, built with ReactJS, allows users to interact with the data stored in the database without modifying the original directories.</li> <li>Virtual Machine Deployment: The entire project is designed to run on a Virtual Machine (VM), which has access to the necessary directories.</li> </ul>"},{"location":"index.html#software-stack","title":"Software Stack","text":"<ul> <li>Database: PostgreSQL (Version 12)</li> <li>API Server: Flask (Version 2.3.2)</li> <li>Web Interface: ReactJS (Version 18.2.0)</li> </ul>"},{"location":"index.html#project-deployment-requirements","title":"Project Deployment Requirements","text":""},{"location":"index.html#system-requirements","title":"System Requirements","text":"<ul> <li>Virtual Machine (VM) with:</li> <li>Operating System: Linux (Ubuntu 22.04.2 LTS)</li> <li>CPU: 4 cores</li> <li>RAM: 8 GB</li> <li>Storage: ~300 GB</li> </ul>"},{"location":"index.html#software-requirements","title":"Software Requirements","text":"<ul> <li>Node.js: Version 18.17.0</li> <li>Python: Version 3.11.4</li> <li>PostgreSQL: Latest version</li> <li>Flask: Version 2.3.2</li> <li>Additional Flask Libraries: <code>flask_cors</code>, <code>psycopg2</code></li> <li>Git: Latest stable version</li> <li>Visual Studio Code: Latest version</li> </ul>"},{"location":"index.html#network-requirements","title":"Network Requirements","text":"<ul> <li>Static IP Address</li> <li>Necessary Ports: HTTP/HTTPS (default 3000), Flask server (default 5000), PostgreSQL (default 5432)</li> </ul>"},{"location":"index.html#backup-and-recovery","title":"Backup and Recovery","text":"<ul> <li>Daily backups of the PostgreSQL database</li> </ul>"},{"location":"index.html#maintenance-and-monitoring","title":"Maintenance and Monitoring","text":"<ul> <li>Regular system monitoring for optimal performance</li> <li>Routine log reviews for potential issues</li> </ul>"},{"location":"index.html#scalability","title":"Scalability","text":"<ul> <li>Designed to handle up to 150,000 samples over the next 10 years.</li> </ul>"},{"location":"index.html#contact-information","title":"Contact Information","text":"<p>For further assistance or inquiries, please contact us at: - Email: thedeepchandra@gmail.com - Phone: +974 66449748</p>"},{"location":"2.%20setup.html","title":"Setup","text":"<p>Welcome to the setup guide for the Sidra BioInformatics Data Platform. This guide will walk you through the steps required to install, configure, and initialize the platform on your system.</p>"},{"location":"2.%20setup.html#prerequisites","title":"Prerequisites","text":"<p>Before you begin, ensure your system meets the following requirements:</p> <ul> <li>Operating System: Linux (Ubuntu 22.04.2 LTS)</li> <li>CPU: 4 cores</li> <li>RAM: 8 GB</li> <li>Storage: ~300 GB</li> </ul>"},{"location":"2.%20setup.html#step-1-install-necessary-software","title":"Step 1: Install Necessary Software","text":""},{"location":"2.%20setup.html#install-nodejs","title":"Install Node.js","text":"<pre><code>curl -fsSL https://deb.nodesource.com/setup_18.x | sudo -E bash -\nsudo apt-get install -y nodejs\n</code></pre>"},{"location":"2.%20setup.html#install-python","title":"Install Python","text":"<pre><code>sudo apt-get update\nsudo apt-get install -y python3.11\n</code></pre>"},{"location":"2.%20setup.html#install-postgresql","title":"Install PostgreSQL","text":"<pre><code>sudo apt-get update\nsudo apt-get install -y postgresql\n</code></pre>"},{"location":"2.%20setup.html#install-git","title":"Install Git","text":"<pre><code>sudo apt-get update\nsudo apt-get install -y git\n</code></pre>"},{"location":"2.%20setup.html#install-visual-studio-code","title":"Install Visual Studio Code","text":"<pre><code>sudo snap install --classic code\n</code></pre>"},{"location":"2.%20setup.html#step-2-clone-the-project-repository","title":"Step 2: Clone the Project Repository","text":"<pre><code>git clone https://github.com/deepchandra02/genomics-lab-dashboard\ncd genomics-lab-dashboard\n</code></pre>"},{"location":"2.%20setup.html#step-3-configure-the-database","title":"Step 3: Configure the Database","text":"<p>To setup the database, first ensure that you have the <code>psycopg2</code> library installed. </p> <pre><code>pip install psycopg2\n</code></pre> <p>Then, open the file <code>database/createDB.py</code> and edit the details initial lines with the corrct configuration details:</p> database/createDB.py<pre><code>conn = psycopg2.connect(database=\"sidra\", # (1)!\nhost=\"localhost\", # (2)!\nuser=\"person\", # (3)!\npassword=\"mypassword\", # (4)!\nport=\"5432\") # (5)!\n</code></pre> <ol> <li>Should be the name of the database you created in this step iv</li> <li>Should be the host name of the database server. If you are running the database server on the same machine as the web server, then this should be <code>localhost</code>.</li> <li>Should match the credentials of the database user you create later in this step ii</li> <li>Should match the credentials of the database user you create later in this step ii</li> <li>Should be the port number of the database server. By default, it is <code>5432</code>.</li> </ol> <p>Then, open the file <code>database/insertDB.py</code> and enter the same configuration details as above.</p> <p>Also, remeber to specify the right directory paths in the <code>database/inserDB.py</code> file:</p> database/insertDB.py<pre><code>  # Set up the paths\ndirectory_path = \"./input-data-for-externs/FC multiqc\"\ndirectory = \"./input-data-for-externs\"\nfcqc_directory = directory + \"/flowcell-qc-reports\"\nrawinfo_directory = directory + \"/rawinfo-dirs\"\nruns_directory = directory + \"/Runs\"\n</code></pre>"},{"location":"2.%20setup.html#step-4-get-the-database-running","title":"Step 4: Get the Database running","text":"<p>Once you have configured the database, you can run the <code>database/script.sh</code> in your terminal to setup the database. Navigate to the <code>database</code> directory in your terminal and execute the following command:</p> <pre><code>chmod +x script.sh\n./script.sh\n</code></pre> <p>Warning</p> <p>You may need to update the PostgreSQL username and password in the <code>script.sh</code> file to match your PostgreSQL installation. Additionally, ensure that the Python scripts <code>createDB.py</code> and <code>insertDB.py</code> are present in the same directory as <code>script.sh</code>.</p> <p>This script is designed to set up the database for the Sidra BioInformatics Data Platform. Here is a breakdown of what this script does:</p>"},{"location":"2.%20setup.html#i-clear-the-terminal-screen","title":"i. Clear the Terminal Screen:","text":"<p>It starts by clearing the terminal screen for a clean view of the commands and their outputs.   <pre><code>clear\n</code></pre></p>"},{"location":"2.%20setup.html#ii-set-database-user-and-password","title":"ii. Set Database User and Password:","text":"<p>The script creates the PostgreSQL user <code>person</code> and with the password <code>'mypassword'</code>.   <pre><code>psql -U postgres -c \"ALTER USER person PASSWORD 'mypassword';\"\n</code></pre></p>"},{"location":"2.%20setup.html#iii-drop-existing-database-if-any","title":"iii. Drop Existing Database (if any):","text":"<p>The script drops the existing <code>sidra</code> database if it already exists. This is to ensure a fresh start.   <pre><code>psql -U person -d template1 -c 'DROP DATABASE IF EXISTS sidra;'\n</code></pre></p>"},{"location":"2.%20setup.html#iv-create-new-database","title":"iv. Create New Database:","text":"<p>The script creates a new PostgreSQL database named <code>sidra</code>.   <pre><code>psql -U person -d template1 -c 'CREATE DATABASE sidra;'\n</code></pre></p>"},{"location":"2.%20setup.html#v-run-the-createdbpy-script","title":"v. Run the <code>createDB.py</code> Script:","text":"<p>This Python script is responsible for setting up the tables and schema of the <code>sidra</code> database.   <pre><code>python3 createDB.py\n</code></pre></p>"},{"location":"2.%20setup.html#vi-run-the-insertdbpy-script","title":"vi. Run the <code>insertDB.py</code> Script:","text":"<p>This Python script is responsible for populating the <code>sidra</code> database with initial data. The output and errors of this script are redirected to a file named <code>log.txt</code>.   <pre><code>python3 insertDB.py &gt; \"log.txt\" 2&gt;&amp;1\n</code></pre></p> <p>Warning</p> <p>Check the <code>log.txt</code> file to ensure that the database was set up successfully. If there are any errors, you can debug them by looking at the error messages in the <code>log.txt</code> file. If there are no errors, you can proceed to the next step.</p>"},{"location":"2.%20setup.html#step-5-setting-up-the-flask-server","title":"Step 5: Setting Up the Flask Server","text":""},{"location":"2.%20setup.html#install-flask-and-required-libraries","title":"Install Flask and Required Libraries","text":"<pre><code>pip install Flask==2.3.2\npip install flask_cors\n</code></pre>"},{"location":"2.%20setup.html#start-the-flask-server","title":"Start the Flask Server","text":"<pre><code>cd api\nexport FLASK_APP=app.py\nflask run\n</code></pre>"},{"location":"2.%20setup.html#step-5-start-the-reactjs-web-interface","title":"Step 5: Start the ReactJS Web Interface","text":"<pre><code>cd web-interface\nnpm install\nnpm start\n</code></pre> <p>Your Sidra BioInformatics Data Platform should now be up and running. Navigate to <code>http://localhost:3000</code> in your web browser to access the web interface.</p>"},{"location":"2.%20setup.html#step-6-network-configuration","title":"Step 6: Network Configuration","text":"<ul> <li>Assign a static IP address to your VM.</li> <li>Configure the necessary ports in your firewall settings.</li> </ul>"},{"location":"2.%20setup.html#step-7-set-up-backups","title":"Step 7: Set Up Backups","text":"<ul> <li>Configure daily backups for your PostgreSQL database.</li> </ul>"},{"location":"2.%20setup.html#step-8-monitoring-and-maintenance","title":"Step 8: Monitoring and Maintenance","text":"<ul> <li>Set up system monitoring tools.</li> <li>Schedule regular log reviews.</li> </ul> <p>Update</p>"},{"location":"2.%20setup.html#contact-for-support","title":"Contact for Support","text":"<p>For further assistance or inquiries, please contact us at: - Email: support@sidrabioinformatics.com - Phone: +1 (234) 567-8901</p>"},{"location":"3.%20database.html","title":"Database","text":""},{"location":"3.%20database.html#overview","title":"Overview","text":"<p>This page provides detailed information about the PostgreSQL database schema in the platform and how it is populated. Two scripts are primarily responsible here:</p> <ol> <li>When the <code>database/createDB.py</code> file is executed in this step, the database schema is created in the PostgreSQL database.</li> <li>When the <code>database/insertDB.py</code> file is executed in this step, it connects to the database created in the previous step, reads data from various files in specified directories (primarily from the <code>raw.info</code> files), processes this data, and inserts it into the appropriate tables in the database.</li> </ol>"},{"location":"3.%20database.html#the-tables","title":"The Tables","text":"<p>Below are the schema details of the various tables in the database, along with the relevant code snippets from the <code>createDB.py</code> and <code>insertDB.py</code> scripts.</p>"},{"location":"3.%20database.html#table-1-i5","title":"Table 1: <code>i5</code>","text":"<p>This table contains information about i5 indices, which are used in the demultiplexing process during sequencing.</p> SchemacreateDB.pyinsertDB.py Attribute Data Type Constraints Description i5_id VARCHAR(16) PRIMARY KEY Unique identifier for i5 index i5_sequence VARCHAR(10) DEFAULT NULL, CHECK (char_length(i5_sequence) &gt;= 06) The sequence of the i5 index <pre><code>sql(\"CREATE TABLE i5 (\\\n      i5_id           VARCHAR(16) PRIMARY KEY,\\\n      i5_sequence     VARCHAR(10) DEFAULT NULL CHECK (char_length(i5_sequence) &gt;= 06)\\\n  );\")\n</code></pre> <pre><code># table 15\ntry:\ncursor.execute(\"INSERT INTO i5 (i5_id, i5_sequence) VALUES ('%s', '%s');\"%(row[\"INDEX_I5_ID\"], row[\"INDEX_I5_sequencing\"])) #ON CONFLICT (i5_id) DO NOTHING\nexcept:\n# print(\"i5_sequence already exists; checking for data integrity...\")\nsql(\"SELECT i5_sequence FROM i5 WHERE i5_id = '%s';\"%(row[\"INDEX_I5_ID\"], ))\ndata = cursor.fetchone()\nif data != None and data[0] != row[\"INDEX_I5_sequencing\"]:\nprint(\"entry with i5_index %s already exists with i5_sequence %s\"%(row[\"INDEX_I5_ID\"], data[0]))\n</code></pre>"},{"location":"3.%20database.html#table-2-i7","title":"Table 2: <code>i7</code>","text":"<p>This table contains information about i7 indices, which are also used in the demultiplexing process during sequencing.</p> SchemacreateDB.pyinsertDB.py Attribute Data Type Constraints Description i7_id VARCHAR(16) PRIMARY KEY Unique identifier for i7 index i7_sequence VARCHAR(10) DEFAULT NULL, CHECK (char_length(i7_sequence) &gt;= 06) The sequence of the i7 index <pre><code>sql(\"CREATE TABLE i7 (\\\n      i7_id           VARCHAR(16) PRIMARY KEY,\\\n      i7_sequence     VARCHAR(10) DEFAULT NULL CHECK (char_length(i7_sequence) &gt;= 06)\\\n  );\")\n</code></pre> <pre><code>#  table 17\ntry:\ncursor.execute(\"INSERT INTO i7 (i7_id, i7_sequence) VALUES ('%s', '%s');\"%(row[\"INDEX_I7_ID\"], row[\"INDEX_I7_sequencing\"])) #ON CONFLICT (i7_id) DO NOTHING\nexcept:\n# print(\"i7_sequence already exists; checking for data integrity...\")\nsql(\"SELECT i7_sequence FROM i7 WHERE i7_id = '%s';\"%(row[\"INDEX_I7_ID\"], ))\ndata = cursor.fetchone()\nif data != None and data[0] != row[\"INDEX_I7_sequencing\"]:\nprint(\"entry with i7_index %s already exists with i7_sequence %s\"%(row[\"INDEX_I7_ID\"], data[0]))\n# else:\n# print(\"data integrity checked for i7\")\n</code></pre>"},{"location":"3.%20database.html#table-3-sequencer","title":"Table 3: <code>sequencer</code>","text":"<p>This table contains information about sequencing machines used in the lab.</p> SchemacreateDB.pyinsertDB.py Attribute Data Type Constraints Description sequencer_id VARCHAR(16) PRIMARY KEY Unique identifier for the sequencer machine <pre><code>sql(\"CREATE TABLE sequencer (\\\n      sequencer_id    VARCHAR(16) PRIMARY KEY\\\n  );\")\n</code></pre> <pre><code># table sequencer\nsql(\"INSERT INTO sequencer (sequencer_id) VALUES ('%s') ON CONFLICT (sequencer_id) DO NOTHING;\"%(row[\"Sequencer\"], ))\n</code></pre>"},{"location":"3.%20database.html#table-4-pi_projects","title":"Table 4: <code>pi_projects</code>","text":"<p>This table contains information about different projects, including the principal investigator associated with each project and the project's requirements and timeline.</p> SchemacreateDB.pyinsertDB.py Attribute Data Type Constraints Description project_id VARCHAR(32) PRIMARY KEY Unique identifier for the project pi VARCHAR(08) NOT NULL Principal Investigator responsible for the project requirement VARCHAR(32) DEFAULT '_' Specific requirements or goals of the project earliest DATE DEFAULT NULL Earliest date associated with the project (e.g., proposal date) latest DATE DEFAULT NULL Latest or end date associated with the project <pre><code>sql(\"CREATE TABLE pi_projects (\\\n      project_id      VARCHAR(32) PRIMARY KEY,\\\n      pi              VARCHAR(08) NOT NULL,\\\n      requirement     VARCHAR(32) DEFAULT '_',\\\n      earliest        DATE DEFAULT NULL,\\\n      latest          DATE DEFAULT NULL\\\n  );\")\n</code></pre> <pre><code># table pi_projects\ntry:\ncursor.execute(\"INSERT INTO pi_projects (project_id, pi, requirement) VALUES ('%s', '%s', '%s');\"%(row[\"Project\"], row[\"PI\"], row[\"Data Requirement\"]))\nexcept:\n# print(\"pi, project pair already exists; checking for data integrity...\")\nsql(\"SELECT requirement FROM pi_projects WHERE project_id = '%s' AND pi = '%s';\"%(row[\"Project\"], row[\"PI\"]))\ndata = cursor.fetchone()\nif data != None and data[0] != row[\"Data Requirement\"]:\nprint(\"data requirement changed for project %s from %s to %s\"%(row[\"Project\"], data[0], row[\"Data Requirement\"]))\n# else:\n# print(\"data integrity checked for pi_project\")\nsql(\"SELECT earliest, latest FROM pi_projects WHERE project_id = '%s';\"%(row[\"Project\"]))\ndata = cursor.fetchone()\nif data != None:\nassert(len(data) == 2)\nif data[0] == None or data[0] &gt; row[\"Submission Date\"]:\nsql(\"UPDATE pi_projects SET earliest = '%s' WHERE project_id = '%s' AND pi = '%s';\"%(row[\"Submission Date\"], row[\"Project\"], row[\"PI\"]))\nif data[1] == None or data[1] &lt; row[\"Submission Date\"]:\nsql(\"UPDATE pi_projects SET latest = '%s' WHERE project_id = '%s' AND pi = '%s';\"%(row[\"Submission Date\"], row[\"Project\"], row[\"PI\"]))\nelse:\nprint(\"date fetch failed\")\n</code></pre>"},{"location":"3.%20database.html#table-5-submissions","title":"Table 5: <code>submissions</code>","text":"<p>This table contains information about project submissions, including the associated project, date of submission, and various metadata related to the sequencing process.</p> SchemacreateDB.pyinsertDB.py Attribute Data Type Constraints Description submission_id VARCHAR(64) PRIMARY KEY Unique identifier for submission project_id VARCHAR(32) REFERENCES pi_projects (project_id) Linked project ID date DATE NOT NULL, CHECK (date &lt;= current_date) Date when the samples were submitted for sequencing cov VARCHAR(16) DEFAULT '_' Coverage target for sequencing (e.g., 100x) srv VARCHAR(16) DEFAULT '_' Type of sequencing service requested (e.g., Whole Genome, RNA-Seq) rg VARCHAR(16) DEFAULT '_' Read Group information for BAM file header anl VARCHAR(16) DEFAULT '_' Type of analysis to be performed on the data (e.g., Variant Calling) datatype VARCHAR(16) NOT NULL Type of data (e.g., DNA, RNA) <pre><code>sql(\"CREATE TABLE submissions (\\\n      submission_id   VARCHAR(64) PRIMARY KEY,\\\n      project_id      VARCHAR(32) REFERENCES pi_projects (project_id),\\\n      date            DATE NOT NULL CHECK (date &lt;= current_date),\\\n      cov             VARCHAR(16) DEFAULT '_',\\\n      srv             VARCHAR(16) DEFAULT '_',\\\n      rg              VARCHAR(16) DEFAULT '_',\\\n      anl             VARCHAR(16) DEFAULT '_',\\\n      datatype        VARCHAR(16) NOT NULL\\\n  );\")\n</code></pre> <pre><code>#  table submissions\ntry:\ncursor.execute(\"INSERT INTO submissions (submission_id, project_id, date, datatype) VALUES ('%s', '%s', '%s', '%s');\"%(row[\"Submission ID\"], row[\"Project\"], row[\"Submission Date\"], row[\"Datatype\"]))\nexcept Exception as e:\nif not str(e).startswith(\"duplicate key value violates\"):\nprint(e)\n# print(\"submission_id already exists; checking for data integrity...\")\nif row[\"Comments\"] != \"\":\ncomments = row[\"Comments\"].split(\" \")\nassert(len(comments) == 5)\nsid = comments[0].split(\":\")\nsrv = comments[1].split(\":\")\nrg = comments[2].split(\":\")\ncov = comments[3].split(\":\")\nanl = comments[4].split(\":\")\nassert(len(sid) == 0 or (len(sid) == 2 and sid[0] == \"SUB\"))\nassert(len(srv) == 0 or (len(srv) == 2 and srv[0] == \"SRV\"))\nassert(len(rg) == 0 or (len(rg) == 2 and rg[0] == \"RG\"))\nassert(len(cov) == 0 or (len(cov) == 2 and cov[0] == \"COV\"))\nassert(len(anl) == 0 or (len(anl) == 2 and anl[0] == \"ANL\"))\nassert(sid[1] == row[\"Submission ID\"])\nsql(\"SELECT srv, rg, cov, anl FROM submissions WHERE submission_id = '%s';\"%(row[\"Submission ID\"]))\ndata = cursor.fetchone()\n# if data != None:\n# assert(len(data) == 4)\nif (data == None or data[0] == None or data[0] == \"_\") and srv[1] != \"\":\nsql(\"UPDATE submissions SET srv = '%s' WHERE submission_id = '%s' ;\"%(srv[1], row[\"Submission ID\"]))\nelif data[0] != srv[1]:\nprint(\"service data changed for submission id '%s' from '%s' to '%s'\"%(row[\"Submission ID\"], data[0], srv[1]))\nif (data == None or data[1] == None or data[1] == \"_\") and rg[1] != \"\":\nsql(\"UPDATE submissions SET rg = '%s' WHERE submission_id = '%s' ;\"%(rg[1], row[\"Submission ID\"]))\nelif data[1] != rg[1]:\nprint(\"ref_genome data changed for submission id '%s' from '%s' to '%s'\"%(row[\"Submission ID\"], data[1], rg[1]))\nif (data == None or data[2] == None or data[2] == \"_\") and cov[1] != \"\":\nsql(\"UPDATE submissions SET cov = '%s' WHERE submission_id = '%s' ;\"%(cov[1], row[\"Submission ID\"]))\nelif data[2] != cov[1]:\nprint(\"coverage data changed for submission id '%s' from '%s' to '%s'\"%(row[\"Submission ID\"], data[2], cov[1]))\nif (data == None or data[3] == None or data[3] == \"_\") and anl[1] != \"\":\nsql(\"UPDATE submissions SET anl = '%s' WHERE submission_id = '%s' ;\"%(anl[1], row[\"Submission ID\"]))\nelif data[3] != anl[1]:\nprint(\"analysis data changed for submission id '%s' from '%s' to '%s'\"%(row[\"Submission ID\"], data[3], anl[1]))\n</code></pre>"},{"location":"3.%20database.html#table-6-flowcell","title":"Table 6: <code>flowcell</code>","text":"<p>This table contains detailed information about flowcells used in sequencing, including the type of flowcell, who loaded it, and various dates associated with the sequencing process.</p> SchemacreateDB.pyinsertDB.py Attribute Data Type Constraints Description fc_id VARCHAR(16) PRIMARY KEY Unique identifier for flowcell fc_type VARCHAR(16) NOT NULL, CHECK (char_length(fc_type) &gt; 0) Type of flowcell (e.g., NovaSeq S4) loaded_by VARCHAR(08) NOT NULL, CHECK (char_length(loaded_by) &gt; 0) Lab member who loaded the flowcell onto the sequencer loading_date DATE NOT NULL, CHECK (loading_date &lt;= current_date) Date when the flowcell was loaded onto the sequencer completion_date DATE NOT NULL, CHECK (completion_date &lt;= current_date), CHECK (completion_date &gt;= loading_date) Date when the sequencing run was completed demultiplex_date DATE NOT NULL, CHECK (demultiplex_date &lt;= current_date) Date when the demultiplexing was completed stage_date DATE DEFAULT NULL, CHECK (stage_date &lt;= current_date) Date when the data was staged for analysis process_date DATE DEFAULT NULL, CHECK (process_date &lt;= current_date) Date when the data processing was completed order_no VARCHAR(32) NOT NULL, CHECK (char_length(order_no) &gt; 0) Order or batch number associated with the sequencing run sequencer_id VARCHAR(16) REFERENCES sequencer (sequencer_id) Linked sequencer ID run_duration SMALLINT CHECK ((run_duration ISNULL) OR (run_duration &gt; 0)) DEFAULT NULL Duration of the sequencing run in hours position BOOLEAN NOT NULL Position of the flowcell on the sequencer (e.g., left or right) <pre><code>sql(\"CREATE TABLE flowcell (\\\n      fc_id           VARCHAR(16) PRIMARY KEY,\\\n      fc_type         VARCHAR(16) NOT NULL CHECK (char_length(fc_type) &gt; 0),\\\n      loaded_by       VARCHAR(08) NOT NULL CHECK (char_length(loaded_by) &gt; 0),\\\n      loading_date    DATE     NOT NULL CHECK (loading_date &lt;= current_date),\\\n      completion_date DATE     NOT NULL CHECK (completion_date &lt;= current_date),\\\n      demultiplex_date DATE    NOT NULL CHECK (demultiplex_date &lt;= current_date),\\\n      stage_date       DATE    DEFAULT NULL CHECK (stage_date &lt;= current_date),\\\n      process_date     DATE    DEFAULT NULL CHECK (process_date &lt;= current_date),\\\n      order_no        VARCHAR(32) NOT NULL CHECK (char_length(order_no) &gt; 0),\\\n      sequencer_id    VARCHAR(16) REFERENCES sequencer (sequencer_id),\\\n      run_duration    SMALLINT CHECK ((run_duration ISNULL) OR (run_duration &gt; 0)) DEFAULT NULL,\\\n      position        BOOLEAN NOT NULL,\\\n      CHECK (completion_date &gt;= loading_date)\\\n  );\")\n</code></pre> <pre><code>try:\n# print(row[\"Completion Date\"])\n# print(row)\ncursor.execute(\"INSERT INTO flowcell (fc_id, fc_type, loaded_by, loading_date, completion_date, demultiplex_date, order_no, sequencer_id, position)\\\n    VALUES ('%s', '%s', '%s', '%s', '%s', '%s', '%s', '%s', '%s');\"%(row[\"FC\"], row[\"FC Type\"], row[\"Loaded By\"], row[\"Loading Date\"], row[\"Completion Date\"], row[\"Demultiplex Date\"], row[\"Order No\"], row[\"Sequencer\"], row[\"Position\"]))\nexcept Exception as e:\n# print(e)\n# print(row[\"Completion Date\"])\n# print(row)\n# print(\"flowcell already exists; checking for data integrity...\")\nsql(\"SELECT fc_type, loaded_by, loading_date, completion_date, demultiplex_date, order_no, sequencer_id, position FROM flowcell WHERE fc_id = '%s' ;\"%(row[\"FC\"])) # try except data integrity\ndata = cursor.fetchone()\nif data != None:\n# print(data)\nassert(len(data) == 8)\ntry:\nloading_date = datetime.datetime.strptime(row[\"Loading Date\"], '%m/%d/%Y').date()\nexcept:\nloading_date = datetime.datetime.strptime(row[\"Loading Date\"], '%Y-%m-%d').date()\ntry:\ncompletion_date = datetime.datetime.strptime(row[\"Completion Date\"], '%m/%d/%Y').date()\nexcept:\ncompletion_date = datetime.datetime.strptime(row[\"Completion Date\"], '%Y-%m-%d').date()\n# demultiplex_date = datetime.datetime.strptime(row[\"Demultiplex Date\", ])\nif data[0] != row[\"FC Type\"]:\nprint(\"FC Type data changed for flowcell %s from %s to %s\"%(row[\"FC\"], data[0], row[\"FC Type\"]))\nif data[1] != row[\"Loaded By\"]:\nprint(\"Loaded By data changed for flowcell %s from %s to %s\"%(row[\"FC\"], data[1], row[\"Loaded By\"]))\nif data[2] != loading_date:\nprint(\"Loading Date data changed for flowcell %s from %s to %s\"%(row[\"FC\"], data[2], loading_date))\nif data[3] != completion_date:\nprint(\"Completed Date data changed for flowcell %s from %s to %s\"%(row[\"FC\"], data[3], completion_date))\nif data[4] != row[\"Demultiplex Date\"]:\nprint(\"Demultiplex Date data changed for flowcell %s from %s to %s\"%(row[\"FC\"], data[4], row[\"Demultiplex Date\"]))\nif data[5] != row[\"Order No\"]:\nprint(\"Order No data changed for flowcell %s from %s to %s\"%(row[\"FC\"], data[5], row[\"Order No\"]))\nif data[6] != row[\"Sequencer\"]:\nprint(\"Sequencer data changed for flowcell %s from %s to %s\"%(row[\"FC\"], data[6], row[\"Sequencer\"]))\nif data[7] != row[\"Position\"]:\nprint(\"position data changed for flowcell %s from %s to %s\"%(row[\"FC\"], data[7], row[\"Position\"]))\nelse:\nprint(\"flowcell data fetch failed\")\n</code></pre>"},{"location":"3.%20database.html#table-7-pools","title":"Table 7: <code>pools</code>","text":"<p>This table contains information about pooled samples, which are mixtures of multiple samples that are sequenced together in a single lane of a flowcell.</p> SchemacreateDB.pyinsertDB.py Attribute Data Type Constraints Description pooling_id VARCHAR(32) PRIMARY KEY Unique identifier for pooling pf_reads VARCHAR(08) NOT NULL Number of pass filter reads obtained from sequencing loading_conc VARCHAR(08) NOT NULL Concentration of the pooled sample loaded onto the flowcell q30 NUMERIC(5, 3) NOT NULL, CHECK ((q30 &gt;= 0) AND (q30 &lt;= 100)) Percentage of bases with a Q30 quality score lane_1 BOOLEAN NOT NULL, DEFAULT FALSE Indicates whether the pool was sequenced in lane 1 lane_2 BOOLEAN NOT NULL, DEFAULT FALSE Indicates whether the pool was sequenced in lane 2 lane_3 BOOLEAN NOT NULL, DEFAULT FALSE Indicates whether the pool was sequenced in lane 3 lane_4 BOOLEAN NOT NULL, DEFAULT FALSE Indicates whether the pool was sequenced in lane 4 fc_id VARCHAR(16) REFERENCES flowcell (fc_id) Linked flowcell ID <pre><code>sql(\"CREATE TABLE pools (\\\n      pooling_id      VARCHAR(32) PRIMARY KEY,\\\n      pf_reads        VARCHAR(08) NOT NULL,\\\n      loading_conc    VARCHAR(08) NOT NULL,\\\n      q30             NUMERIC(5, 3) NOT NULL CHECK ((q30 &gt;= 0) AND (q30 &lt;= 100)),\\\n      lane_1          BOOLEAN  NOT NULL DEFAULT FALSE,\\\n      lane_2          BOOLEAN  NOT NULL DEFAULT FALSE,\\\n      lane_3          BOOLEAN  NOT NULL DEFAULT FALSE,\\\n      lane_4          BOOLEAN  NOT NULL DEFAULT FALSE,\\\n      fc_id           VARCHAR(16) REFERENCES flowcell (fc_id)\\\n);\")\n</code></pre> <pre><code># table pools\ntry:\ncursor.execute(\"INSERT INTO pools (pooling_id, pf_reads, loading_conc, q30, fc_id) VALUES ('%s', '%s', '%s', '%s', '%s');\"%(row[\"Pooling ID\"], row[\"Reads (PF)\"], row[\"Loading Conc.\"], row[\"Q30\"], row[\"FC\"]))\nexcept Exception as e:\n# print(e)\n# print(\"pools data already exists; checking for data integrity...\")\nsql(\"SELECT pf_reads, loading_conc, q30, fc_id FROM pools WHERE pooling_id = '%s';\"%(row[\"Pooling ID\"])) # try except data integrity\ndata = cursor.fetchone()\nif data != None:\nassert(len(data) == 4)\nif data[0] != row[\"Reads (PF)\"]:\nprint(\"pf_reads data changed for pool '%s' from '%s' to '%s'\"%(row[\"Pooling ID\"], data[0], row[\"Reads (PF)\"]))\nif data[1] != row[\"Loading Conc.\"]:\nprint(\"loading_conc data changed for pool '%s' from '%s' to '%s'\"%(row[\"Pooling ID\"], data[1], row[\"Loading Conc.\"]))\nif float(data[2]) != float(row[\"Q30\"]):\nprint(\"q30 data changed for pool '%s' from '%s' to '%s'\"%(row[\"Pooling ID\"], data[2], row[\"Q30\"]))\nif data[3] != row[\"FC\"]:\nprint(\"fc_id data changed for pool '%s' from '%s' to '%s'\"%(row[\"Pooling ID\"], data[3], row[\"FC\"]))\nelse:\nprint(\"pool data fetch failed\")\nsql(\"SELECT lane_1, lane_2, lane_3, lane_4 FROM pools where pooling_id = '%s';\"%(row[\"Pooling ID\"]))\ndata = cursor.fetchone()\nif data != None:\nassert(len(data) == 4)\nif data[0] == False and row[\"Lane\"] == \"L1\":\nsql(\"UPDATE pools SET lane_1 = TRUE WHERE pooling_id = '%s';\"%(row[\"Pooling ID\"]))\nelif data[1] == False and row[\"Lane\"] == \"L2\":\nsql(\"UPDATE pools SET lane_2 = TRUE WHERE pooling_id = '%s';\"%(row[\"Pooling ID\"]))\nif data[2] == False and row[\"Lane\"] == \"L3\":\nsql(\"UPDATE pools SET lane_3 = TRUE WHERE pooling_id = '%s';\"%(row[\"Pooling ID\"]))\nif data[3] == False and row[\"Lane\"] == \"L4\":\nsql(\"UPDATE pools SET lane_4 = TRUE WHERE pooling_id = '%s';\"%(row[\"Pooling ID\"]))\nelse:\nprint(\"pool lane data fetch failed\")\n</code></pre>"},{"location":"3.%20database.html#custom-type-status","title":"Custom Type: <code>status</code>","text":"<p>This custom type defines the possible statuses for a sample, indicating whether the sample is new, a top-up of a previous sample, or a repeat. The <code>status</code> type is used in the <code>samples</code> table. </p> createDB.py <pre><code>CREATE TYPE status AS ENUM ('New', 'Top-up', 'Repeat');\n</code></pre>"},{"location":"3.%20database.html#table-8-samples","title":"Table 8: <code>samples</code>","text":"<p>This table contains detailed information about individual samples, including the associated pooling and flowcell information, quality control metrics, and various metadata related to the sequencing process.</p> SchemacreateDB.pyinsertDB.py Attribute Data Type Constraints Description sample_id VARCHAR(10) NOT NULL, CHECK (char_length(sample_id) = 10) Unique identifier for sample pooling_id VARCHAR(32) REFERENCES pools (pooling_id) Linked pooling ID fc_id VARCHAR(16) REFERENCES flowcell (fc_id) Linked flowcell ID sample_name VARCHAR(64) NOT NULL Human-readable name of the sample submission_id VARCHAR(64) REFERENCES submissions (submission_id) Linked submission ID qpcr NUMERIC(4, 2) CHECK ((qpcr ISNULL) OR (qpcr &gt;= 0)) DEFAULT NULL QPCR concentration of the sample fragment SMALLINT NOT NULL, CHECK (fragment &gt;= 0) Fragment size of the library in base pairs labchip_conc NUMERIC(5, 2) CHECK ((labchip_conc ISNULL) OR (labchip_conc &gt;= 0)) DEFAULT NULL Concentration measured by LabChip well VARCHAR(32) NOT NULL, CHECK (char_length(well) &gt; 0) Well position in the plate where the sample is located pre_norm_well VARCHAR(32) DEFAULT '_' Well position before normalization i5_id VARCHAR(16) REFERENCES i5 (i5_id) Linked i5 index ID i7_id VARCHAR(16) REFERENCES i7 (i7_id) Linked i7 index ID data_sample status NOT NULL Status of the sample (New, Top-up, Repeat) urgent BOOLEAN NOT NULL, DEFAULT FALSE Flag indicating whether the sample is marked as urgent remark text DEFAULT '_' Additional remarks or notes about the sample lib_received DATE NOT NULL, CHECK (lib_received &lt;= current_date) Date when the library was received for sequencing sample_qc BOOLEAN NOT NULL, DEFAULT FALSE Quality control status of the sample lib_qc BOOLEAN NOT NULL, DEFAULT FALSE Quality control status of the library error VARCHAR(64) NOT NULL, DEFAULT '_' Details of any errors associated with the sample stage_date DATE DEFAULT NULL, CHECK (stage_date &lt;= current_date) Date when the sample data was staged for analysis merged BOOLEAN DEFAULT FALSE Flag indicating whether the sample data was merged from multiple lanes lane BOOLEAN DEFAULT FALSE Flag indicating whether the sample was run in a dedicated lane PRIMARY KEY - (sample_id, fc_id) Composite primary key based on sample ID and flowcell ID <pre><code>sql(\"CREATE TABLE samples (\\\n      sample_id       VARCHAR(10) NOT NULL CHECK (char_length(sample_id) = 10),\\\n      pooling_id      VARCHAR(32) REFERENCES pools (pooling_id),\\\n      fc_id           VARCHAR(16) REFERENCES flowcell (fc_id),\\\n      sample_name     VARCHAR(64) NOT NULL,\\\n      submission_id   VARCHAR(64) REFERENCES submissions (submission_id),\\\n      qpcr            NUMERIC(4, 2) CHECK ((qpcr ISNULL) OR (qpcr &gt;= 0)) DEFAULT NULL,\\\n      fragment        SMALLINT NOT NULL CHECK (fragment &gt;= 0),\\\n      labchip_conc    NUMERIC(5, 2) CHECK ((labchip_conc ISNULL) OR (labchip_conc &gt;= 0)) DEFAULT NULL,\\\n      well            VARCHAR(32) NOT NULL CHECK (char_length(well) &gt; 0),\\\n      pre_norm_well   VARCHAR(32) DEFAULT '_',\\\n      i5_id           VARCHAR(16) REFERENCES i5 (i5_id),\\\n      i7_id           VARCHAR(16) REFERENCES i7 (i7_id),\\\n      data_sample     status NOT NULL,\\\n      urgent          BOOLEAN NOT NULL DEFAULT FALSE,\\\n      remark          text DEFAULT '_',\\\n      lib_received    DATE NOT NULL CHECK (lib_received &lt;= current_date),\\\n      sample_qc       BOOLEAN NOT NULL DEFAULT FALSE,\\\n      lib_qc          BOOLEAN NOT NULL DEFAULT FALSE,\\\n      error           VARCHAR(64) NOT NULL DEFAULT '_',\\\n      stage_date       DATE    DEFAULT NULL CHECK (stage_date &lt;= current_date),\\\n      merged          BOOLEAN DEFAULT FALSE,\\\n      lane            BOOLEAN DEFAULT FALSE,\\\n      PRIMARY KEY (sample_id, fc_id)\\\n  );\")\n</code></pre> <pre><code># table samples\nsql(\"SELECT pooling_id, sample_name, submission_id, qpcr, fragment, labchip_conc, well, pre_norm_well, i5_id, i7_id, data_sample, urgent, remark, lib_received, sample_qc, lib_qc\\\n FROM samples WHERE sample_id = '%s' AND fc_id = '%s';\"%(row[\"Sample Name\"], row[\"FC\"]))\ndata = cursor.fetchone()\nif data == None:\nsql(\"INSERT INTO samples (sample_id, pooling_id, fc_id, sample_name, submission_id, fragment, well, pre_norm_well, i5_id, i7_id, data_sample, urgent, remark, lib_received, sample_qc, lib_qc)\\\n    VALUES ('%s','%s','%s','%s','%s','%s','%s','%s','%s','%s','%s','%s','%s','%s','%s', '%s' );\"%(row[\"Sample Name\"], row[\"Pooling ID\"], row[\"FC\"], row[\"Original Sample Name\"], row[\"Submission ID\"], row[\"Fragment size (bp)\"], row[\"Well\"], row[\"Pre-Norm Well\"], row[\"INDEX_I5_ID\"], row[\"INDEX_I7_ID\"], row[\"Data_Sample_Status\"],\nrow[\"Urgency\"], row[\"Remark\"], row[\"Libaries and info received date\"], row[\"Sample QC P/F\"], row[\"Lib QC P/F\"]))\nelse:\nassert(len(data) == 16)\nif data[0] != row[\"Pooling ID\"]:\nprint(\"Pooling ID data changed for sample '%s' and fc '%s' from '%s' to '%s'\"%(row[\"Sample Name\"], row[\"FC\"], data[0], row[\"Pooling ID\"]))\nif data[1] != row[\"Original Sample Name\"]:\nprint(\"Original Sample Name data changed for sample '%s' and fc '%s' from '%s' to '%s'\"%(row[\"Sample Name\"], row[\"FC\"], data[1], row[\"Original Sample Name\"]))\nif data[2] != row[\"Submission ID\"]:\nprint(\"Submission ID data changed for sample '%s' and fc '%s' from '%s' to '%s'\"%(row[\"Sample Name\"], row[\"FC\"], data[2], row[\"Submission ID\"]))\nif float(data[3]) != float(row[\"QPCR Conc. (nM) / iseq output\"]):\nprint(\"QPCR Conc. (nM) / iseq output data changed for sample '%s' and fc '%s' from '%s' to '%s'\"%(row[\"Sample Name\"], row[\"FC\"], data[3], row[\"QPCR Conc. (nM) / iseq output\"]))\nif int(data[4]) != int(row[\"Fragment size (bp)\"]):\nprint(\"Fragment size (bp) data changed for sample '%s' and fc '%s' from '%s' to '%s'\"%(row[\"Sample Name\"], row[\"FC\"], data[4], row[\"Fragment size (bp)\"]))\nif float(data[5]) != float(row[\"LabChip/Bioanalyzer Conc. (nM)\"]):\nprint(\"LabChip/Bioanalyzer Conc. (nM) data changed for sample '%s' and fc '%s' from '%s' to '%s'\"%(row[\"Sample Name\"], row[\"FC\"], data[5], row[\"LabChip/Bioanalyzer Conc. (nM)\"]))\nif data[6] != row[\"Well\"]:\nprint(\"Well data changed for sample '%s' and fc '%s' from '%s' to '%s'\"%(row[\"Sample Name\"], row[\"FC\"], data[6], row[\"Well\"]))\nif data[7] != row[\"Pre-Norm Well\"]:\nprint(\"Pre-Norm Well data changed for sample '%s' and fc '%s' from '%s' to '%s'\"%(row[\"Sample Name\"], row[\"FC\"], data[7], row[\"Pre-Norm Well\"]))\nif data[8] != row[\"INDEX_I5_ID\"]:\nprint(\"INDEX_i5_ID data changed for sample '%s' and fc '%s' from '%s' to '%s'\"%(row[\"Sample Name\"], row[\"FC\"], data[8], row[\"INDEX_i5_ID\"]))\nif data[9] != row[\"INDEX_I7_ID\"]:\nprint(\"INDEX_i7_ID data changed for sample '%s' and fc '%s' from '%s' to '%s'\"%(row[\"Sample Name\"], row[\"FC\"], data[9], row[\"INDEX_i7_ID\"]))\nif data[10] != row[\"Data_Sample_Status\"]:\nprint(\"Data_Sample_status data changed for sample '%s' and fc '%s' from '%s' to '%s'\"%(row[\"Sample Name\"], row[\"FC\"], data[10], row[\"Data_Sample_status\"]))\nif data[11] != row[\"Urgency\"]:\nprint(\"Urgency data changed for sample '%s' and fc '%s' from '%s' to '%s'\"%(row[\"Sample Name\"], row[\"FC\"], data[11], row[\"Urgency\"]))\nif data[12] != row[\"Remark\"]:\nprint(\"Remark data changed for sample '%s' and fc '%s' from '%s' to '%s'\"%(row[\"Sample Name\"], row[\"FC\"], data[12], row[\"Remark\"]))        \nif data[13] != datetime.datetime.strptime(row[\"Libaries and info received date\"], \"%Y%m%d\").date():\nprint(\"Libaries and info received date data changed for sample '%s' and fc '%s' from '%s' to '%s'\"%(row[\"Sample Name\"], row[\"FC\"], data[13], row[\"Libaries and info received date\"]))\nif data[14] != row[\"Sample QC P/F\"]:\nprint(\"Sample QC P/F data changed for sample '%s' and fc '%s' from '%s' to '%s'\"%(row[\"Sample Name\"], row[\"FC\"], data[14], row[\"Sample QC P/F\"]))\nif data[15] != row[\"Lib QC P/F\"]:\nprint(\"Lib QC P/F data changed for sample '%s' and fc '%s' from '%s' to '%s'\"%(row[\"Sample Name\"], row[\"FC\"], data[15], row[\"Lib QC P/F\"]))\nif row[\"QPCR Conc. (nM) / iseq output\"] != \"\":\nsql(\"UPDATE samples SET qpcr = '%s' WHERE sample_id = '%s' AND fc_id = '%s';\"%(row[\"QPCR Conc. (nM) / iseq output\"], row[\"Sample Name\"], row[\"FC\"]))\nif row[\"LabChip/Bioanalyzer Conc. (nM)\"] != \"\":\nsql(\"UPDATE samples SET labchip_conc = '%s' WHERE sample_id = '%s' AND fc_id = '%s';\"%(row[\"LabChip/Bioanalyzer Conc. (nM)\"], row[\"Sample Name\"], row[\"FC\"]))\n</code></pre>"},{"location":"3.%20database.html#erd","title":"ERD","text":"<pre><code>erDiagram\n    pi_projects ||--o{ submissions : \"1..n\"\n    pi_projects {\n        project_id VARCHAR(32)\n        pi VARCHAR(08)\n        requirement VARCHAR(32)\n        earliest DATE\n        latest DATE\n    }\n\n    i5 ||--o{ samples : \"1..n\"\n    i5 {\n        i5_id VARCHAR(16)\n        i5_sequence VARCHAR(10)\n    }\n\n    i7 ||--o{ samples : \"1..n\"\n    i7 {\n        i7_id VARCHAR(16)\n        i7_sequence VARCHAR(10)\n    }\n\n    sequencer ||--o{ flowcell : \"1..n\"\n    sequencer {\n        sequencer_id VARCHAR(16)\n    }\n\n    submissions ||--o{ samples : \"1..n\"\n    submissions {\n        submission_id VARCHAR(64)\n        project_id VARCHAR(32)\n        date DATE\n        cov VARCHAR(16)\n        srv VARCHAR(16)\n        rg VARCHAR(16)\n        anl VARCHAR(16)\n        datatype VARCHAR(16)\n    }\n\n    flowcell ||--o{ pools : \"1..n\"\n    flowcell ||--o{ samples : \"1..n\"\n    flowcell {\n        fc_id VARCHAR(16)\n        fc_type VARCHAR(16)\n        loaded_by VARCHAR(08)\n        loading_date DATE\n        completion_date DATE\n        demultiplex_date DATE\n        stage_date DATE\n        process_date DATE\n        order_no VARCHAR(32)\n        sequencer_id VARCHAR(16)\n        run_duration SMALLINT\n        position BOOLEAN\n    }\n\n    pools {\n        pooling_id VARCHAR(32)\n        pf_reads VARCHAR(08)\n        loading_conc VARCHAR(08)\n        q30 NUMERIC\n        lane_1 BOOLEAN\n        lane_2 BOOLEAN\n        lane_3 BOOLEAN\n        lane_4 BOOLEAN\n        fc_id VARCHAR(16)\n    }\n\n    samples {\n        sample_id VARCHAR(10)\n        pooling_id VARCHAR(32)\n        fc_id VARCHAR(16)\n        sample_name VARCHAR(64)\n        submission_id VARCHAR(64)\n        qpcr NUMERIC\n        fragment SMALLINT\n        labchip_conc NUMERIC\n        well VARCHAR(32)\n        pre_norm_well VARCHAR(32)\n        i5_id VARCHAR(16)\n        i7_id VARCHAR(16)\n        data_sample ENUM\n        urgent BOOLEAN\n        remark TEXT\n        lib_received DATE\n        sample_qc BOOLEAN\n        lib_qc BOOLEAN\n        error VARCHAR(64)\n        stage_date DATE\n        merged BOOLEAN\n        lane BOOLEAN\n    }</code></pre>"},{"location":"3.%20database.html#the-process","title":"The Process","text":"<p>The PostgreSQL database currently has 35 attributes overall. The directories having the files from which the data is extracted and populated in the database is to be configured during setup in this step. The directories are:</p> database/insertDB.py<pre><code>  # Set up the paths\ndirectory_path = \"./input-data-for-externs/FC multiqc\"\ndirectory = \"./input-data-for-externs\"\nfcqc_directory = directory + \"/flowcell-qc-reports\"\nrawinfo_directory = directory + \"/rawinfo-dirs\"\nruns_directory = directory + \"/Runs\"\n</code></pre> <p>The following is a detailed walkthrough of the database population process from the <code>insertDB.py</code> script.</p>"},{"location":"3.%20database.html#main-function","title":"main Function","text":"<p>The <code>main</code> function is the entry point of the script. It:</p> <ol> <li>Finds all the HTML files in the flowcell subdirectories under <code>flowcell-qc-reports</code> directory. </li> <li>Extracts the flowcell IDs from these filenames and stores them in a list <code>FC</code>.</li> </ol> <p>Danger</p> <p>The date it is found and the path for the corresponding HTML file for every FC is saved in the database.</p> <p><pre><code>def main():\nFC = []\nsubdirectories = [entry.name for entry in os.scandir(fcqc_directory) if entry.is_dir()]\nfor dir in subdirectories:\nsubdir = fcqc_directory + \"/\" + dir\nhtml_files = glob.glob(os.path.join(subdir, \"*.html\"))\nif html_files:\nfor html_file_path in html_files:\nprint(datetime.datetime.now(), \" : Found HTML file generated for .\", html_file_path)\nfc = html_file_path.split(\"/\")[-1]\nFC.append(fc.split(\".\")[0])\n</code></pre> 3. Queries the database to find which of these flowcells have already been demultiplexed and stores their IDs in a set <code>demultiplexed_FC</code>. <pre><code>    sql(\"SELECT fc_id FROM flowcell;\")\ndata = cursor.fetchall()\ndemultiplexed_FC = set()\nfor row in data:\ndemultiplexed_FC.add(row[0])\n</code></pre> 4. For each flowcell ID in <code>FC</code> that is not in <code>demultiplexed_FC</code>, calls the <code>store_fc</code> function to process and store the data for that flowcell. <pre><code>    for fc in FC:\nif fc not in demultiplexed_FC:\nstore_fc(fc)\nprint(datetime.datetime.now(), \" : Data stored in the database for fc\" + fc)\nreturn 0\nmain()\n</code></pre></p>"},{"location":"3.%20database.html#store_fc-function","title":"store_fc Function","text":"<p>The <code>store_fc</code> function is the core of the script. It processes data related to a specific flowcell (<code>fc</code>). For each flowcell, it: 1. Finds the corresponding raw information file from a directory of raw information files. 2. Reads and processes the data from this file. 3. Constructs a dictionary (<code>row</code>) for each line in the file, where the keys are field names and the values are the data for those fields. 4. Modifies and cleans this data as necessary (e.g., parsing dates, converting strings to booleans, etc.). 5. Calls the <code>store_row</code> function (not shown in the script) to insert this processed data into the appropriate tables in the database.</p> <pre><code>def store_fc(fc):\n...\nfor line in raw_info_file:\nrow = dict()\n...\nstore_row(row)\n</code></pre>"},{"location":"3.%20database.html#the-process_1","title":"The Process:","text":"<ol> <li> <p>Connecting to the Database: The script starts by establishing a connection to the PostgreSQL database and creating a cursor object to interact with the database.</p> </li> <li> <p>Reading and Processing Files: For each flowcell, the script reads the corresponding raw information file, processes this data, and constructs a dictionary for each line in the file.</p> </li> <li> <p>Data Cleaning and Transformation: The script performs various data cleaning and transformation tasks, such as parsing dates, converting strings to booleans, and renaming, adding, or removing fields as necessary.</p> </li> <li> <p>Storing Data in the Database: After processing the data for each flowcell, the script calls a separate <code>store_row</code> function to insert this data into the appropriate tables in the database.</p> </li> <li> <p>Checking for Existing Data: Before inserting data for a flowcell, the script checks whether data for that flowcell has already been demultiplexed and stored in the database. If so, it skips that flowcell to avoid duplicate data.</p> </li> <li> <p>Logging and Error Handling: Throughout the script, there are print statements that log the progress of the script, and exception handling that prints errors when SQL commands fail to execute.</p> </li> </ol> <p>This script is designed to be run periodically to update the database with new data as new flowcells are processed and new raw information files are generated.</p>"},{"location":"4.%20flask_api.html","title":"Flask API Documentation","text":""},{"location":"4.%20flask_api.html#overview","title":"Overview","text":"<p>The API is a Flask application that provides various endpoints to query and retrieve sequencing data from a PostgreSQL database. The API is designed to be consumed by client applications and supports Cross-Origin Resource Sharing (CORS). The file is under <code>sidra/api.py</code></p>"},{"location":"4.%20flask_api.html#endpoints","title":"Endpoints","text":""},{"location":"4.%20flask_api.html#get","title":"<code>GET /</code>","text":"<ul> <li>Description: A simple endpoint to check if the API is running.</li> <li>Response: A plain text response with the message 'HELLO'.</li> </ul>"},{"location":"4.%20flask_api.html#get-type1date","title":"<code>GET /type1/&lt;date&gt;</code>","text":"<ul> <li>Description: Retrieves the number of samples and flowcells processed between a given date range.</li> <li>Parameters:</li> </ul>"},{"location":"4.%20flask_api.html#i-date-a-string-in-the-format-yyyymmdd-yyyymmdd-representing-the-start-and-end-date","title":"i. <code>date</code>: A string in the format 'yyyymmdd-yyyymmdd' representing the start and end date.","text":"<ul> <li>Response: A JSON array of objects, each containing:</li> <li><code>date</code>: The date of demultiplexing.</li> <li><code>Samples</code>: The number of unique samples processed on that date.</li> <li><code>Flowcells</code>: The number of unique flowcells processed on that date.</li> <li><code>SamplesTotal</code>: The cumulative total of unique samples processed up to that date.</li> <li><code>FlowcellsTotal</code>: The cumulative total of unique flowcells processed up to that date.</li> </ul>"},{"location":"4.%20flask_api.html#get-type2adate","title":"<code>GET /type2a/&lt;date&gt;</code>","text":"<ul> <li>Description: Retrieves the number of samples processed for each PI and data sample type between a given date range.</li> <li>Parameters:</li> <li><code>date</code>: A string in the format 'yyyymmdd-yyyymmdd' representing the start and end date.</li> <li>Response: A JSON array of objects, each containing:</li> <li><code>pi</code>: The name of the PI.</li> <li><code>New</code>: The number of new samples for this PI.</li> <li><code>Top-up</code>: The number of top-up samples for this PI.</li> <li><code>Repeat</code>: The number of repeat samples for this PI.</li> </ul>"},{"location":"4.%20flask_api.html#get-type2bdate","title":"<code>GET /type2b/&lt;date&gt;</code>","text":"<ul> <li>Description: Retrieves the number of samples processed for each PI and project between a given date range.</li> <li>Parameters:</li> <li><code>date</code>: A string in the format 'yyyymmdd-yyyymmdd' representing the start and end date.</li> <li>Response: A JSON array of objects, each containing:</li> <li><code>pi</code>: The name of the PI.</li> <li><code>project_id</code>: The ID of the project.</li> <li><code>quantity</code>: The number of samples processed for this project.</li> </ul>"},{"location":"4.%20flask_api.html#get-type2cdate","title":"<code>GET /type2c/&lt;date&gt;</code>","text":"<ul> <li>Description: Retrieves detailed information about samples processed between a given date range.</li> <li>Parameters:</li> <li><code>date</code>: A string in the format 'yyyymmdd-yyyymmdd' representing the start and end date.</li> <li>Response: A JSON array of objects, each containing:</li> <li><code>pi</code>: The name of the PI.</li> <li><code>project</code>: The ID of the project.</li> <li><code>sample_count</code>: The number of samples processed for this project.</li> <li><code>genome</code>: The reference genome used for this project.</li> </ul>"},{"location":"4.%20flask_api.html#get-type3date","title":"<code>GET /type3/&lt;date&gt;</code>","text":"<ul> <li>Description: Retrieves the number of flowcells processed of each type between a given date range.</li> <li>Parameters:</li> <li><code>date</code>: A string in the format 'yyyymmdd-yyyymmdd' representing the start and end date.</li> <li>Response: A JSON array of objects, each containing:</li> <li><code>type</code>: The type of flowcell.</li> <li><code>quantity</code>: The number of flowcells of this type processed.</li> </ul>"},{"location":"4.%20flask_api.html#get-type4date","title":"<code>GET /type4/&lt;date&gt;</code>","text":"<ul> <li>Description: Retrieves the number of samples processed for each service type between a given date range.</li> <li>Parameters:</li> <li><code>date</code>: A string in the format 'yyyymmdd-yyyymmdd' representing the start and end date.</li> <li>Response: A JSON array of objects, each containing:</li> <li><code>type</code>: The type of service.</li> <li><code>quantity</code>: The number of samples processed for this service type.</li> </ul>"},{"location":"4.%20flask_api.html#get-type5date","title":"<code>GET /type5/&lt;date&gt;</code>","text":"<ul> <li>Description: Retrieves the number of samples processed on each sequencer between a given date range.</li> <li>Parameters:</li> <li><code>date</code>: A string in the format 'yyyymmdd-yyyymmdd' representing the start and end date.</li> <li>Response: A JSON array of objects, each containing:</li> <li><code>type</code>: The ID of the sequencer.</li> <li><code>quantity</code>: The number of samples processed on this sequencer.</li> </ul>"},{"location":"4.%20flask_api.html#get-type6date","title":"<code>GET /type6/&lt;date&gt;</code>","text":"<ul> <li>Description: Retrieves the number of samples processed for each reference genome between a given date range.</li> <li>Parameters:</li> <li><code>date</code>: A string in the format 'yyyymmdd-yyyymmdd' representing the start and end date.</li> <li>Response: A JSON array of objects, each containing:</li> <li><code>type</code>: The name of the reference genome.</li> <li><code>quantity</code>: The number of samples processed for this reference genome.</li> </ul>"},{"location":"4.%20flask_api.html#running-the-api","title":"Running the API","text":"<p>To run the API, execute the script <code>sidra/api.py</code>. The API will run in debug mode and will be accessible at <code>http://localhost:5000/</code> by default.</p>"},{"location":"4.%20flask_api.html#error-handling","title":"Error Handling","text":"<p>If the date parameter is not in the expected format, the API will return a plain text response with the message \"format should be 'yyyymmdd-yyyymmdd'\".</p>"},{"location":"4.%20flask_api.html#note","title":"Note","text":"<p>Make sure that the PostgreSQL database is running and accessible with the credentials provided in the script.</p>"}]}